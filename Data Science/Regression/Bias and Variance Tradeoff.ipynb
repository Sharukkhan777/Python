{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source :\n",
    "\n",
    "https://www.youtube.com/watch?v=EuBBz3bI-aA&pbjreload=101\n",
    "\n",
    "Simple Words:\n",
    "=============\n",
    "RSS = Residuals Sum of Squares\n",
    "\n",
    "Bias_train = RSS_train_data \n",
    "Bias_test = RSS_test_data \n",
    "\n",
    "Variance = abs( RSS_train_data - RSS_test_data )\n",
    "\n",
    "Explanation:\n",
    "============\n",
    "* Suppose we are taking the Height(X) and Weight(Y) of the Mice\n",
    "* Splits the data into train and test\n",
    "* First we use linear regression( aka Least Squares) to fit the training data\n",
    "* So Linear Regression fits the straight line to the Training data\n",
    "* So there is more chance to get Residuals( the perpendicular distance between true and predicted points)\n",
    "* ---Bias---\n",
    "    The inability for a machine learning method to capture the real relationship is called Bias.\n",
    "    (The above definition of bias simply means, Here we fit the Linear Line so we can see the residuals for 99% cases, The residuals means bias. If the residuals is high then the bias is also high)\n",
    "* Another Machine Learning method might fit a Squiggly Line.\n",
    "* In the way it fit as No Residuals( Zero perpendicular distance between true and predicted points)\n",
    "* ----Bias----\n",
    "    Here the Bias is too low, since the squiggly line perfectly fits the data.\n",
    "    \n",
    "==== Bias Conclusion ====\n",
    "\n",
    "Here in the two cases\n",
    "\n",
    "Case 1: Linear Line\n",
    "    Bias is high for Linear Regression.\n",
    "    \n",
    "Case 2: Squiggly Line\n",
    "    Bias is Low for Squiggly Line.\n",
    "==========================\n",
    "\n",
    "* Now we calculate Residuals sum of square for -test data-\n",
    "\n",
    "* ---- Variance ----\n",
    "    The difference between the fits of training and test is called Variance.\n",
    "    ==> How to calculate:\n",
    "         i)Calculate Residuals sum of square(perpendicular distance between predict and real) for train and test data.\n",
    "         \n",
    "         ii) Variance = abs( RSS_train_data - RSS_test_data )\n",
    "    \n",
    "    \n",
    "* Case 2: Squiggly Line\n",
    "    In training data the Squiggly Line fits too good.\n",
    "    When we fit for test data it doesn't fit good(did bad job on test data)\n",
    "    So the high difference in fit between train and test\n",
    "    \n",
    "    (Low Bias High Variance)\n",
    "    \n",
    "* Case 1: Linear Regression\n",
    "    In training data the Linear Line fit better.\n",
    "    When we fit for test data it do better fit.\n",
    "    So there is no much difference in fit between train and test data.\n",
    "    \n",
    "    (High Bias Low Variance)\n",
    "    \n",
    "    \n",
    "    \n",
    "Terminology Alert:\n",
    "==================\n",
    "\n",
    "    i) OverFit : \n",
    "    The Squiggly Line super fit for training data and worst fit for test data is called Overfit.\n",
    "    \n",
    "    ii) Regularization, Bagging and Boosting:\n",
    "    The commonly used methods to find the \n",
    "    Low Bias and Low Variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
