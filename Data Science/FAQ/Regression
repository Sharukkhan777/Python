Questions:
1. What if 4 variables out of 9 variables is linear relationship, Can we use Linear regression?
2. Exogeneity?

------Feature Selection------
3. Best Feature Selection method?
4. Feature Selection, how to do, in what ways?
5. How to Choose the method of Feature Selection?
6. Website for Feature Selection?
7. Lasso Regression as Feature Selection? Yes, we can perform.

8. Difference between Ridge and Lasso?
9. How to control bias and variance tradeoff?
10. Why use Regularization?








Answers:
#######################################################################################################
1. What if 4 variables out of 9 variables is linear relationship, Can we use Linear regression?
    -> First how to find linear relationship?
          Simple Scatter plot to X1,y   X2,y   X3,y    Xn,y , 
          We can see some linear relationship by those scatter plot,
    -> In Real time data,
          We never use one model to fit,
          We use linear, polynomial, random forest and so on
          So we see accuracy of each model and -select- which has good accuracy
    -> Flagging
          So the question is Can we use Linear Regression or not
          Simply Out of 9 variables, if we have 5 or more varibles
          has linear relationship(high majority)
          Then we can use linear regression.
    >>>> CODE :  https://github.com/Sharukkhan777/Python/blob/master/Data%20Science/Regression/Linear%20Regression/Linear%20Relationship%20with%20Dep%20var%20Y.ipynb
#######################################################################################################
2. Exogeneity?

    -> Simple Terms

        * First, We can find this in the assumptions of Regression.
        * In the Regression Equation,
            y = mx + b + ε (error term, epsilon)
        * If we Scatter plot x and ε(error).
            If we find any correlation, whether it is positive or negative,
            The Linear Regression will be biased one.

    ->With Example 
        * Suppose y is Ice-Cream Sales
                  x is Temperature
                  ε is error term
        * Generally we know y and x is positively correlated.
        * But if we Scatter plot Ice-Cream Sales and ε(error).
            If we find any correlation, whether it is positive or negative,
            The Linear Regression will be biased one.

            #>>>>>>CODE = 
        
#######################################################################################################
3. Best Feature Selection method?
    There is no best machine learning algorithm 
    same like that 
    there is no best Feature Selection method
    All depends on the data.
        
        
 #######################################################################################################
 4. Feature Selection, how to do, in what ways?
 #>>>>>>>>>>Source : https://machinelearningmastery.com/feature-selection-machine-learning-python/
    1. Univariate Selection
        * Statistical tests can be used to select those features that have
          the strongest relationship with the output variable.
        * Univariate naming simply implies, at a time one by one indep var compared to one dep var.
        * ANOVA, correlation co-eff,...
    2. Recursive Feature Elimination(RFE)
        * Tells which -combination- works good.
        * For both Categorical and Numerical.
    3. Principal Component Analysis(Unsupervised) and some other technique
    4. Feature Importance
        * Giving score for each variable
#######################################################################################################
5. Choose the method of Feature Selection?
    #>>>>>>>>>>Source : https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/
    # >>>>> image address : https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/11/How-to-Choose-Feature-Selection-Methods-For-Machine-Learning.png
    So when you see the image above you might wonder 
    
    ===> For Mixed Independent Variables contains both Cate and Nume vars!!!!
        2 ways
        * Separate Cate and do feature selection, 
          Separate Nume and do feature selection,
          Combine both
        * RFE method...(powerful technique for feature selection)
          #>>>>>>>>>>Source : https://machinelearningmastery.com/rfe-feature-selection-in-python/
#######################################################################################################
6. Website for Feature Selection?
    > Feature Selection for Machine Learning
        https://machinelearningmastery.com/feature-selection-machine-learning-python/
    > How to Choose a Feature Selection Method For Machine Learning
        https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/
    > Recursive Feature Elimination (RFE) for Feature Selection in Python
        https://machinelearningmastery.com/rfe-feature-selection-in-python/

#######################################################################################################
8. Difference between Ridge and Lasso?
    Ridge regression shrinks the coefficients for those predictors which contribute very less in 
    the model but have huge weights, very close to zero. But it never makes them exactly zero. 
    Thus, the final model will still contain all those predictors, though with less weights. 
    This doesn’t help in interpreting the model very well. This is where Lasso regression differs 
    with Ridge regression. In Lasso, the L1 penalty does reduce some coefficients exactly to zero 
    when we use a sufficiently large tuning parameter λ. So, in addition to regularizing, lasso 
    also performs feature selection.
#######################################################################################################
9. How to control bias and variance tradeoff?
    * Using Regularization we can control, see Question , Why use Regularization.
#######################################################################################################
10. Why use Regularization?
    Regularization helps to reduce the variance of the model, without a substantial increase in the
    bias. If there is variance in the model that means that the model won’t fit well for dataset
    different that training data. The tuning parameter λ controls this bias and variance tradeoff.
    When the value of λ is increased up to a certain limit, it reduces the variance without losing
    any important properties in the data. But after a certain limit, the model will start losing some
    important properties which will increase the bias in the data. Thus, the selection of good value
    of λ is the key. The value of λ is selected using cross-validation methods. A set of λ is 
    selected and cross-validation error is calculated for each value of λ and that value of λ is 
    selected for which the cross-validation error is minimum.
11.When should you use plain Linear Regression (i.e., without any regularization), Ridge, Lasso, or Elastic Net?
    According to the Hands-on Machine Learning book, it is almost always preferable to have at least a little bit of
    regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect 
    that only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the 
    useless features’ weights down to zero as we have discussed. In general, Elastic Net is preferred over Lasso since
    Lasso may behave erratically when the number of features is greater than the number of training instances or when 
    several features are strongly correlated.
